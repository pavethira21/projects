The project aims at building a Image Captioning system that will
automatically generate the caption for the given input image. It
involves challenging task of generating a human readable textual description of an image. 
It involves natural language processing and
computer vision. These caption helps the user to know the essential
information of the image.
The system is built using Deep Neural network models. The
model is divided into two parts the encoder and the decoder. The
encoder extracts the features from the image , the last layer of the
encoder is connected to the decoder which generate a caption for the
image. Pretrained CNN and RNN models are used to build the model. From the feature
vectors obtained from models such as VGG19 and inception-V3 the
LSTM model generate sequence of words.
The dataset used to train the model is flickr8k dataset. This
dataset consists of 8091 images from flickr and each image has 5
captions that describe the actions in the image .

DATASET
The dataset used for this study is flickr8k dataset which consists
about 8091 images with each image having 5 captions that provide
clear descriptions of the salient entities and events. The Data set
was complied by P.Young, M.Hodosh and J.Hockenmaier. This
dataset is preprocessed and then applied in the model.
 *Pre-processing
1. Image Dataset
The images are loaded by using load img method after loading all
the image the image pixels are then converted into array as the
model can only process an array value and not a pixel value, then the
images are reshaped into 4d image.The images are further processed
by using a predefined function preprocess input .
2. Caption Data
The caption are loaded and then each caption is separated from its
index value and stored in a dictionary with its index as key. All
the words in the captions are separated and stored. The words are
written in lower case using the function .lower() for better quality
All the special character and numbers that are present in the captions are removed along with extra spacing.
Then finally the words are bounded by a start and end tag to indicate the starting and
ending of the caption.

Feature Extraction
The image data are preprocessed and then fed into CNN models
to extract their features. The Models used here are VGG19 and
Inception-V3 models. The models are imported from keras.applications.
Vgg19 and Inception-V3 are pretrained models that enable us to extract feature with more accuracy. the last to layers of the models(the
prediction and the classification layer) are removed while loadeing
the model as they are not essential for our research purpose then
the images are fed and their features are extracted.After extracted
the features from the image the features are saves in a pickle file for
future use.

Model Creation
The model has two segments which is the encoder model and the
decoder model. The encoder model has two different layers .A dense
layer get the features extracted from the image dataset using CNN
models and the input size of the layer is 4096 and 2048 which are the
output shape of VGG19 model and inception-V3 model respectively.
The activation used in this layer is relu with a dropout of 4%. The
other layer is where LSTM model is implemented.To create a representation of the captions embedding is used with a dropout of 4%.
The decoder combine the two different layers of the encoder and the output layer is a dense layer with softmax as activation. The
cost function used for the model is categorical crossentropy and the
optimizer used is adams.

Interpretation and Conclution
The Bleu score for two of the models has been recorded and the
model that was trained with features extracted from the VGG19
model had a better bleu score compared to the other. Both the
model are executed for about 30 epochs which ran for more than 3
hours.From the above observation we can note that the model that has
been trained with features extracted from inception-V3 model capture minute details in the caption for example in fig:10 we can note
that the caption generated by using VGG19 model generated a caption stating a dog is running through grass while in fig:9 the caption
generated by the model using inception has specified the details like
the dogâ€™s tongue is out .From the comparison table we can conclude that the model trained using features extracted from VGG19 model generates better captions with a overall BLEU-1 score of 0.603293.The model using features from VGG19 model is trained for 25 epochs and while evaluating the loss was noted as 2.1475 and the model using features
from inception-V3 model was trained for 30 epochs and the loss was
noted as 2.0073.
